import asyncio
import math
import os

from numpy import number
from openai import AsyncOpenAI

# ========================================
# Set up the API key for OpenAI
# ========================================

# Initialise the asynchronous OpenAI client by passing the system-wide API key.

async_client = AsyncOpenAI(
    api_key=os.environ.get('OPENAI_API_KEY'),
)

# ========================================
# Asynchronous Chat prototype
# ========================================

# This is a generic prototype of the Asynchronous Chat client. The `system`
# and `user` prompts are passed to the constructor but the response is
# generated only upon calling the `get_response()` method.

class AsyncChat:
    """
    This is a generic prototype of the Asynchronous Chat client. The `system`
    and `user` prompts are passed to the constructor but the response is
    generated only upon calling the `get_response()` method.
    """
    def __init__(self, system: str, user: str):
        """
        Initialise an Asynchronous Chat client.
        :param system: The `system` prompt.
        :param user: The `user` prompt.
        """
        self.system = system
        self.user = user

    async def get_response(self) -> str:
        """
        Generate the response based on the constructing parameters by calling
        some asynchronous API.
        :return: The generated response.
        """
        pass

# ========================================
# Asynchronous GPT Backend
# ========================================

class AsyncGPTBackend(AsyncChat):
    """
    Use GPT (unspecified model version) as the backend for Asynchronous Chat.
    """
    def __init__(self, system: str, user: str, model: str,
                 temperature: int =0, seed: int = 50014,
                 logprobs: bool = False, top_logprobs: int = None):
        """
        Initialise an Asynchronous Chat client with GPT as the backend.
        :param system: The `system` prompt.
        :param user: The `user` prompt.
        :param model: The OpenAI model (usually GPT) to be used.
        :param temperature: The sampling temperature, defaults to 0.
        :param seed: The seed for the random number generator, defaults to 50014.
        :param logprobs: Whether log probabilities should be returned, defaults to False.
        :param top_logprobs: The number of top log probabilities to return, defaults to None.
        """
        super().__init__(system, user)
        self.temperature = temperature
        self.seed = seed
        self.model = model
        self.logprobs = logprobs
        self.top_logprobs = top_logprobs

    async def api_call(self):
        """
        Interact with the ChatGPT API using user-defined parameters.
        :return: A single GPT response.
        """
        return await async_client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": self.system
                }, {
                    "role": "user",
                    "content": self.user
                }
            ],
            model=self.model,
            temperature=self.temperature,
            seed=self.seed,
            logprobs=self.logprobs,
            top_logprobs=self.top_logprobs
        )

    async def get_response(self):
        """
        Get response based on the constructing parameters.
        :return: A string of the top result generated by GPT.
        """
        chat_completion = await self.api_call()
        return chat_completion.choices[0].message.content

class AsyncGPT3BackendHighTemp(AsyncGPTBackend):
    """
    GPT3 Asynchronous Chat client with high temperature (1).
    """
    def __init__(self, system: str, user: str, temperature: int = 1,
                 seed: int = 50014, logprobs: bool = False,
                 top_logprobs: int = None):
        super().__init__(system, user, "gpt-3.5-turbo-0125",
                         temperature, seed, logprobs, top_logprobs)

class AsyncGPT3BackendLowTemp(AsyncGPTBackend):
    """
    GPT3 Asynchronous Chat client with low temperature (0.2).
    """
    def __init__(self, system: str, user: str, temperature: int = 0.2,
                 seed: int = 50014, logprobs: bool = False,
                 top_logprobs: int = None):
        super().__init__(system, user, "gpt-3.5-turbo-0125",
                         temperature, seed, logprobs, top_logprobs)

class AsyncGPT3ProbBackendLowTemp(AsyncGPT3BackendLowTemp):
    def __init__(self, system: str, user: str, temperature: int = 0.2, seed: int = 50014,
                 logprobs: bool = True, top_logprobs: int = 2):
        super().__init__(system, user, temperature, seed, logprobs, top_logprobs)

    async def get_response(self):
        chat_completion = await super().api_call()
        txt = chat_completion.choices[0].message.content
        logprob = chat_completion.choices[0].logprobs.content[0].logprob
        return txt, math.exp(logprob)



# ========================================
# Asynchronous Chat Queues
# ========================================

def slice_into_parts(lst, n):
    """
    Slice a list into n parts.
    :param lst: A list to be sliced.
    :param n: The number of parts.
    :return: A list of lists that add up to the original length.
    """
    length = len(lst)
    base_size = length // n
    remainder = length % n
    parts = []
    start = 0
    for i in range(n):
        end = start + base_size + (1 if i < remainder else 0)
        parts.append(lst[start:end])
        start = end
    return parts



class AsyncChatQueue:
    """
    A task queue for asynchronous chats.
    """
    def __init__(self, chats: list[AsyncChat], concurrent_n: int = 2):
        """
        Initialise the asynchronous task queue.
        :param chats:  A list of AsyncChat objects.
        :param concurrent_n: The number of concurrent operations at the same
        time. Defaults to the value of 2.
        """
        self.operations: list[AsyncChat] = chats
        self.total_n = len(self.operations)
        self.concurrent_n = concurrent_n

    async def get_response(self):
        """
        Get the responses based on the `system` and `user` prompts.
        :return: A list of responses.
        """

        # Slice the chats into concurrent operations.
        slices: list[list[AsyncChat]] = slice_into_parts(self.operations,
                                                         self.concurrent_n)

        async def subroutine(counter: int, sublist: list[AsyncChat]):
            res = []
            for i in range(len(sublist)):
                res.append(await sublist[i].get_response())
            return res

        list_of_lists = await asyncio.gather(*(subroutine(j, counter) for j, counter in enumerate(slices)))
        return [item for sublist in list_of_lists for item in sublist]
